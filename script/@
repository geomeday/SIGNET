# slurm.conf file generated by configurator.html.
# Put this file on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
SlurmctldHost=bell-adm
#SlurmctldHost=
#
AuthType=auth/munge
#CheckpointType=checkpoint/none
CryptoType=crypto/munge
#DisableRootJobs=NO
EnforcePartLimits=YES
Epilog=/etc/slurm/epilog.d/*
#EpilogSlurmctld=
#FirstJobId=1
#MaxJobId=999999
GresTypes=gpu
#GroupUpdateForce=0
#GroupUpdateTime=600
#JobCheckpointDir=/var/slurm/checkpoint
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
#JobFileAppend=0
#JobRequeue=1
JobSubmitPlugins=lua
#KillOnBadExit=0
#LaunchType=launch/slurm
#Licenses=foo*4,bar
#MailProg=/bin/mail
MaxJobCount=50000
#MaxStepCount=40000
MaxTasksPerNode=128
MpiDefault=none
#MpiParams=ports=#-#
#PluginDir=
#PlugStackConfig=
#PrivateData=jobs
ProctrackType=proctrack/cgroup
Prolog=/etc/slurm/prolog.d/*
PrologFlags=x11,contain
#PrologSlurmctld=
#PropagatePrioProcess=0
#PropagateResourceLimits=
#PropagateResourceLimitsExcept=
RebootProgram=/usr/site/rcac/sbin/slurm_reboot.sh
ReturnToService=1
#SallocDefaultCommand=
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmctldPort=6817
SlurmdPidFile=/var/run/slurmd.pid
SlurmdPort=6818
SlurmdSpoolDir=/var/spool/slurm
SlurmUser=slurm
#SlurmdUser=root
#SrunEpilog=
#SrunProlog=
StateSaveLocation=/var/spool/slurm/slurm.state
SwitchType=switch/none
#TaskEpilog=
TaskPlugin=affinity,cgroup
#TaskPluginParam=
TaskProlog=/etc/slurm/slurm-task-prolog
TopologyPlugin=topology/none
#TmpFS=/tmp
#TrackWCKey=no
#TreeWidth=
#UnkillableStepProgram=
#UsePAM=0
#
#
# TIMERS
#BatchStartTimeout=10
#CompleteWait=0
#EpilogMsgTime=2000
#GetEnvTimeout=2
HealthCheckInterval=600
HealthCheckProgram=/usr/sbin/nhc
HealthCheckNodeState=ANY
InactiveLimit=0
KillWait=30
MessageTimeout=30
#ResvOverRun=0
MinJobAge=300
#OverTimeLimit=0
SlurmctldTimeout=120
SlurmdTimeout=3600
UnkillableStepTimeout=300
#VSizeFactor=0
Waittime=0
#
#
# SCHEDULING
#
# Set Defmem at 1.5G, so jobs with unspecified memory can use all cores on a node
# Set Maxmem at 2G so higher memory requests will cost cores.
DefMemPerCPU=1992
MaxMemPerCPU=1992
#SchedulerTimeSlice=30
SchedulerType=sched/backfill
SchedulerParameters=bf_interval=120,bf_window=10800,bf_resolution=300,bf_max_job_test=2000,bf_max_job_assoc=50,bf_continue
#SelectType=select/cons_res
SelectType=select/cons_tres
SelectTypeParameters=CR_Core_Memory
#
#
# JOB PRIORITY
#PriorityFlags=
PriorityType=priority/multifactor
PriorityDecayHalfLife=7-00:00:00
PriorityCalcPeriod=00:05:00
PriorityFavorSmall=NO
PriorityMaxAge=7-00:00:00
PriorityUsageResetPeriod=NONE
PriorityWeightAge=500
PriorityWeightFairshare=50000
PriorityWeightJobSize=1
PriorityWeightPartition=100000
PriorityWeightQOS=500
#
#
# LOGGING AND ACCOUNTING
AccountingStorageEnforce=associations,limits,qos,safe
AccountingStorageHost=bell-adm
#AccountingStorageLoc=
#AccountingStoragePass=
#AccountingStoragePort=
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageUser=slurm
AccountingStoreJobComment=YES
AccountingStorageTRES=gres/gpu
ClusterName=bell
#DebugFlags=
#JobCompHost=
JobCompLoc=/var/log/slurm/job_comp.log
#JobCompPass=
#JobCompPort=
JobCompType=jobcomp/filetxt
#JobCompUser=
#JobContainerType=job_container/none
JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/linux
SlurmctldDebug=3
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdDebug=3
SlurmdLogFile=/var/log/slurm/slurmd.log
#SlurmSchedLogFile=
#SlurmSchedLogLevel=
#
#
# POWER SAVE SUPPORT FOR IDLE NODES (optional)
#SuspendProgram=
#ResumeProgram=
#SuspendTimeout=
ResumeTimeout=1200
#ResumeRate=
#SuspendExcNodes=
#SuspendExcParts=
#SuspendRate=
#SuspendTime=
#
#
# COMPUTE NODES
NodeName=bell-a[000-480] RealMemory=257000 MemSpecLimit=512 CPUs=128 Sockets=2 CoresPerSocket=64 ThreadsPerCore=1 Weight=1000 Feature=A,a State=UNKNOWN
NodeName=bell-b[000-007] RealMemory=1031000 MemSpecLimit=512 CPUs=128 Sockets=8 CoresPerSocket=16 ThreadsPerCore=1 Weight=2000 Feature=B,b State=UNKNOWN
#NodeName=bell-fe[00-03] RealMemory=515000 MemSpecLimit=512 CPUs=20 Sockets=2 CoresPerSocket=10 ThreadsPerCore=1 Feature=FE,fe State=UNKNOWN
NodeName=bell-g[000-003] RealMemory=257000 MemSpecLimit=512 CPUs=128 Sockets=2 CoresPerSocket=64 ThreadsPerCore=1 Gres=gpu:2 Feature=G,g,MI50,mi50 State=UNKNOWN
NodeName=bell-g004 CPUs=48 Sockets=2 CoresPerSocket=24 ThreadsPerCore=1 RealMemory=353903 Gres=gpu:6 Feature=G,g,MI60,mi60 State=UNKNOWN

# Partitions
PartitionName=bell-nodes Nodes=bell-a[000-479],bell-b[000-007] Default=NO DefaultTime=30:00 State=UP MaxCPUsPerNode=128 PriorityJobFactor=1
PartitionName=bell-a Nodes=bell-a[000-479] Default=NO DefaultTime=30:00 State=UP MaxCPUsPerNode=128 DefMemPerCPU=1992 MaxMemPerCPU=1992 PriorityJobFactor=10000
PartitionName=bell-b Nodes=bell-b[000-007] Default=NO DefaultTime=30:00 State=UP MaxCPUsPerNode=128 DefMemPerCPU=8000 MaxMemPerCPU=8000 PriorityJobFactor=10000
PartitionName=bell-g Nodes=bell-g[000-003] Default=NO DefaultTime=30:00 State=UP MaxCPUsPerNode=128 DefMemPerCPU=1992 MaxMemPerCPU=1992 PriorityJobFactor=10000
# bell-x -- bell-g004 is significantly different from other g-nodes
PartitionName=bell-x Nodes=bell-g004 Default=NO DefaultTime=30:00 State=UP MaxCPUsPerNode=48 DefMemPerCPU=7350 MaxMemPerCPU=7350 PriorityJobFactor=10000
PartitionName=bell-standby Nodes=bell-a[000-479] Default=YES DefaultTime=30:00 State=UP MaxCPUsPerNode=128 DefMemPerCPU=1992 MaxMemPerCPU=1992 PriorityJobFactor=1
PartitionName=testpbs Nodes=bell-a[000-479],bell-b[000-007] Default=No DefaultTime=30:00 Hidden=YES State=UP MaxCPUsPerNode=128
#PartitionName=bell-fe Nodes=bell-fe[00-03] Default=NO DefaultTime=30:00 Hidden=YES AllocNodes=gateway State=UP
